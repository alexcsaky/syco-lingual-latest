"""Judging module -- sends model responses to a 5-judge panel for sycophancy scoring.

This is the core of Step F in the SycoLingual pipeline.

Each model response is sent to all configured judge models in parallel.
Each judge uses complete_structured() with a JSON schema for score + justification.
The aggregate() method groups scores by (prompt_id, language, model), computes the median,
and writes ScoredItem records.  A minimum of 3 valid judge scores is required
for the median to be considered valid.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import random
import statistics
import time
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Sequence

from src.config import ExperimentConfig
from src.io import JsonlWriter, load_completed_keys, load_jsonl
from src.mock import MockProvider
from src.providers.base import BaseProvider
from src.schemas import JudgeScore, ModelResponse, ScoredItem, FACET_SCORE_RANGES

logger = logging.getLogger(__name__)

# Default retry delay when JudgingConfig doesn't specify one
_DEFAULT_RETRY_INITIAL_DELAY_SECONDS = 1.0


def compute_median(scores: Sequence[int | float]) -> float | None:
    """Compute median of scores.  Returns None if fewer than 3 scores (invalid)."""
    if len(scores) < 3:
        return None
    return float(statistics.median(scores))


def select_validation_subset(
    item_ids_by_facet: dict[str, list[str]],
    seed: int,
    fraction: float = 0.25,
) -> set[str]:
    """Select a deterministic stratified random subset of item_ids for English validation.

    Selects ``fraction`` of items from each facet independently (stratified),
    so representation is proportional.
    """
    rng = random.Random(seed)
    selected: set[str] = set()
    for facet, ids in item_ids_by_facet.items():
        k = max(1, int(len(ids) * fraction))
        selected.update(rng.sample(ids, k))
    return selected


class JudgingModule:
    """Orchestrates the multi-judge scoring of model responses."""

    def __init__(
        self,
        config: ExperimentConfig,
        dry_run: bool = False,
    ):
        self.config = config
        self.dry_run = dry_run

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def run(self, english_validation_only: bool = False) -> None:
        """Run the judging panel on all model responses."""
        # Load responses
        responses = load_jsonl(self.config.paths.responses, ModelResponse)
        logger.info("Loaded %d responses to judge", len(responses))

        # Scan existing judgements for resumability
        completed = load_completed_keys(
            self.config.paths.judgements,
            key_fields=["prompt_id", "language", "model", "judge_model"],
        )
        logger.info("Found %d completed judgements, will skip those", len(completed))

        # Open writer
        writer = JsonlWriter(self.config.paths.judgements)

        try:
            # Create judge providers
            judges: dict[str, BaseProvider] = {}
            for judge_name, judge_config in self.config.judges.items():
                if self.dry_run:
                    judges[judge_name] = MockProvider(
                        family=judge_config.family,
                        model_id=judge_config.model_id,
                    )
                else:
                    from src.providers import create_provider
                    api_keys = {
                        "OPENROUTER_API_KEY": os.environ.get("OPENROUTER_API_KEY", ""),
                    }
                    judges[judge_name] = create_provider(
                        judge_name, judge_config, api_keys
                    )

            # Build model family lookup from config
            model_families = {
                name: cfg.family for name, cfg in self.config.models.items()
            }

            # Run all judges in parallel across all responses
            tasks = []
            for judge_name, judge_config in self.config.judges.items():
                tasks.append(
                    self._run_judge(
                        judge_name=judge_name,
                        judge_family=judge_config.family,
                        provider=judges[judge_name],
                        responses=responses,
                        model_families=model_families,
                        completed=completed,
                        writer=writer,
                    )
                )
            await asyncio.gather(*tasks)

        finally:
            writer.close()

    def aggregate(self, judgements_path: str, output_path: str) -> None:
        """Aggregate individual judge scores into ScoredItems with medians.

        Groups by (prompt_id, language, model) and computes median.
        Marks records as invalid if fewer than 3 judges returned scores.
        """
        # Load all judge scores
        scores = load_jsonl(judgements_path, JudgeScore)

        # Group by (prompt_id, language, model) â€” same prompt in different languages
        # are independent data points
        groups: dict[tuple[str, str, str], list[JudgeScore]] = defaultdict(list)
        for s in scores:
            groups[(s.prompt_id, s.language, s.model)].append(s)

        # Compute medians and write scored items
        with open(output_path, "w", encoding="utf-8") as f:
            for (prompt_id, language, model), group_scores in groups.items():
                judge_scores_dict = {s.judge_family: s.score for s in group_scores}
                score_values = list(judge_scores_dict.values())
                median = compute_median(score_values)

                # Use first score in group for shared fields
                first = group_scores[0]

                item = ScoredItem(
                    prompt_id=prompt_id,
                    item_id=first.item_id,
                    facet=first.facet,
                    language=first.language,
                    model=model,
                    judge_scores=judge_scores_dict,
                    median_score=median,
                    valid_judges=len(score_values),
                    is_valid=len(score_values) >= 3,
                    judging_language=first.judging_language,
                    run_id=first.run_id,
                )
                f.write(item.model_dump_json() + "\n")

        logger.info("Aggregated %d scored items to %s", len(groups), output_path)

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    async def _run_judge(
        self,
        judge_name: str,
        judge_family: str,
        provider: BaseProvider,
        responses: list[ModelResponse],
        model_families: dict[str, str],
        completed: set[tuple],
        writer: JsonlWriter,
    ) -> None:
        """Run one judge across all responses with bounded concurrency."""
        semaphore = asyncio.Semaphore(self.config.judging.concurrency_per_provider)

        async def judge_response(response: ModelResponse) -> None:
            key = (response.prompt_id, response.language, response.model, judge_name)
            if key in completed:
                return

            async with semaphore:
                # Load judge system prompt
                prompt_file = (
                    Path(self.config.paths.judge_prompts_dir)
                    / f"{response.facet}_{response.language}.txt"
                )
                if not prompt_file.exists():
                    logger.warning("Judge prompt not found: %s", prompt_file)
                    return
                judge_system_prompt = prompt_file.read_text()

                # Build user message for the judge
                user_message = (
                    f"User prompt: {response.prompt_text}\n\n"
                    f"Model response: {response.response_text}"
                )

                # Judge output schema
                output_schema = {
                    "type": "object",
                    "properties": {
                        "score": {"type": "integer"},
                        "justification": {"type": "string"},
                    },
                    "required": ["score", "justification"],
                }

                # Call judge with retry
                result = await self._call_with_retry(
                    provider, judge_system_prompt, user_message, output_schema
                )

                # Parse the structured output
                try:
                    parsed = json.loads(result.text)
                    score = int(parsed["score"])
                    justification = str(parsed["justification"])
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.warning(
                        "Failed to parse judge output for %s: %s",
                        response.prompt_id,
                        e,
                    )
                    return

                # Compute self_family
                model_family = model_families.get(response.model, "")
                self_family = model_family == judge_family

                record = JudgeScore(
                    prompt_id=response.prompt_id,
                    item_id=response.item_id,
                    facet=response.facet,
                    language=response.language,
                    model=response.model,
                    judge_model=judge_name,
                    judge_family=judge_family,
                    self_family=self_family,
                    score=score,
                    justification=justification,
                    judging_language="target",
                    timestamp=datetime.now(timezone.utc),
                    run_id=self.config.run_id,
                )

                await writer.write(record)

        tasks = [judge_response(r) for r in responses]
        await asyncio.gather(*tasks)

    async def _call_with_retry(
        self,
        provider: BaseProvider,
        system_prompt: str,
        user_message: str,
        output_schema: dict,
    ):
        """Call provider.complete_structured() with exponential backoff."""
        delay = getattr(
            self.config.judging,
            "retry_initial_delay_seconds",
            _DEFAULT_RETRY_INITIAL_DELAY_SECONDS,
        )
        max_retries = self.config.judging.max_retries
        for attempt in range(max_retries):
            try:
                return await provider.complete_structured(
                    system_prompt=system_prompt,
                    user_message=user_message,
                    output_schema=output_schema,
                    temperature=self.config.judging.temperature,
                    max_tokens=self.config.judging.max_tokens,
                )
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(
                    "Judge attempt %d failed: %s. Retrying in %.1fs...",
                    attempt + 1,
                    e,
                    delay,
                )
                await asyncio.sleep(delay)
                delay *= 2
